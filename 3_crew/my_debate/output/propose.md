The need for strict laws to regulate Large Language Models (LLMs) stems from the potential risks they pose to society, economy, and individual rights. LLMs, when left unchecked, can propagate misinformation, infringe on privacy, and contribute to bias and discrimination. 

Firstly, misinformation created by LLMs can spread rapidly, resulting in significant public harm. For example, unregulated generative models may produce fake news articles or harmful content that misleads the public, undermining democratic processes and eroding trust in information sources. Establishing strict regulations can ensure that LLMs are supervised and refined to prevent the spread of false information.

Secondly, privacy concerns arise with the usage of LLMs that process vast amounts of personal data. Without clear laws, users' personal information can be exploited, resulting in breaches of privacy and trust. Regulations can mandate data handling protocols, ensuring that user data is protected and only utilized with consent, thus enhancing the ethical use of technology. 

Moreover, bias is an inherent risk in LLMs as they are trained on datasets that may reflect societal prejudices. This can result in discriminatory outputs that affect marginalized communities adversely. Strict regulations can enforce fairness protocols in the development and deployment of LLMs, compelling developers to prioritize inclusivity and equity.

In summary, strict laws are essential for regulating LLMs to prevent the spread of misinformation, protect users' privacy, and eliminate bias. These regulations will safeguard not just individuals but society as a whole, ensuring LLM technology is developed and used responsibly and ethically. Therefore, adopting strict laws to regulate LLMs is imperative for fostering a safe and equitable digital future.